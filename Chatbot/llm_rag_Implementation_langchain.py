# -*- coding: utf-8 -*-
"""LLM RAG for Capstone--Latest

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c4VJMQ-c-Zrg2m2IJ2Oe6ztGUyCM_RIF
"""

pip install langchain

pip install langchain_community

pip install sentence-transformers

pip install faiss-gpu

pip install faiss-cpu langchain transformers sentence-transformers

import json
from pathlib import Path
from langchain.docstore.document import Document
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from tqdm import tqdm
import random

class EmbeddingManager:
    def __init__(self, model_name):
        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)

    def get_embeddings(self):
        return self.embeddings

class VectorSpaceManager:
    def __init__(self, embedding_manager):
        self.embedding_manager = embedding_manager
        self.embeddings = self.embedding_manager.get_embeddings()

    def create_vector_space(self, documents):
        print("Embedding documents...")
        vector_store = FAISS.from_documents(documents, self.embeddings)
        return vector_store

    def save_vector_space(self, vector_store, save_path):
        print(f"Saving vector space to {save_path}...")
        vector_store.save_local(save_path)
        print(f"Finished!")

    def load_vector_space(self, save_path):
        print(f"Loading vector space from {save_path}...")
        return FAISS.load_local(save_path, self.embeddings, allow_dangerous_deserialization=True)

class DataLoader:
    def __init__(self, json_file_path):
        self.json_file_path = json_file_path

    def load_data(self):
        data = json.loads(Path(self.json_file_path).read_text())
        return data

    def create_documents(self, length=None):
        data = self.load_data()
        conversations = data['conversations']

        if length is None:
            length = len(conversations)

        documents = [
            Document(
                page_content=self.get_page_content(item),
                metadata=item
            )
            for item in conversations[:length]
        ]
        return documents

    def get_page_content(self, item):
        raise NotImplementedError("Subclasses must implement get_page_content method")

#Concatenate the Intent, User Queries and Bot Responses into a single string
class ConversationDataLoader(DataLoader):
    def get_page_content(self, item):
        return f"Intent: {item['intent']} | User Queries: {' '.join(item['user_queries'])} | Bot Responses: {' '.join(item['bot_response'])}"
#Load create and save vector space
def process_data(json_file_path, model_name, save_path, data_loader_class, length=None):
    embedding_manager = EmbeddingManager(model_name)
    vector_space_manager = VectorSpaceManager(embedding_manager)

    data_loader = data_loader_class(json_file_path)
    documents = data_loader.create_documents(length=length)
    vector_store = vector_space_manager.create_vector_space(documents)
    vector_space_manager.save_vector_space(vector_store, save_path)

#Performs Similarity search in the vector store using user query and extract the first response
def converse_with_bot(user_query, vector_store):
    search_results = vector_store.similarity_search(user_query, k=1)
    if search_results:
        response_list = search_results[0].metadata['bot_response']
        response = random.choice(response_list)
        return response
    else:
        return "I'm sorry, I couldn't find a relevant response."

if __name__ == "__main__":
    json_file_path = '/content/JSONConversation.json'
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    save_path = '/content/migration_vector_space/'

    process_data(json_file_path, model_name, save_path, ConversationDataLoader, 100)

    vector_store = VectorSpaceManager(EmbeddingManager(model_name)).load_vector_space(save_path)

    while True:
        user_query = input("User: ")
        if user_query.lower() in ['exit', 'quit']:
            break
        bot_response = converse_with_bot(user_query, vector_store)
        print(f"Assistant: {bot_response}")

